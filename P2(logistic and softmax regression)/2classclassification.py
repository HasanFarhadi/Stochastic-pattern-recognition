# -*- coding: utf-8 -*-
"""2ClassClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10nEParvEPL2spilZQzWeftVRdEvsINFm
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

#Loading the iris dataset and removing the third and fourth features
data = pd.read_csv("/content/drive/MyDrive/Depo/PatternHW/HW2/iris.data", header = None).to_numpy()
data = np.delete(data, [2,3], 1)

#Removing the Iris Versicolor samples 
versicolorindex = np.where(data[:, 2] == "Iris-versicolor")
data = np.delete(data, versicolorindex, 0)

#Assigning 0 to setosa and 1 to virginica sampoles 
for i in range(len(data)):
    if (data[i, 2] == 'Iris-virginica'):
        data[i, 2] = 1
    else:
        data[i, 2] = 0

#separating our database into train and test portions
train_features, test_features, train_labels, test_labels = train_test_split(data[:,0:2], data[:,2], test_size= 0.2, stratify= data[:,2], random_state=42)
#assigning initial values to our model parameters
x = train_features
y = train_labels
m = len(x)
n = x.shape[1]

#turning y into a column vector
y = np.reshape(y, (80,1))

#mapping value's into a zero to one range
def Normal(x):
    x = (x - x.min()) / (x.max() - x.min())
    return x

x = Normal(x)
#Appending a one column vector to our features in order to calculate the y-intercept
x = np.append(np.ones((m,1)), x, axis= 1)

#Assigning more initial values to our model parameters
theta = np.ones((3,1))
iterations = 1000
learning_rate = 0.0001
yhat = np.zeros((m, 1))
virginica = []
setosa = []
Loss_Error = []

#sigmoid function for defining h(theta)
def Sigmoid(x):
    sig = 1 / (1 + np.exp(-x.astype(float)))
    return sig

#the logarithm of the Binary cross entropoy Loss function
def LogCost(yhat, y):
    L = -np.mean(y * np.log(yhat.astype(float)) + (1 - y) * np.log((1 - yhat).astype(float)))
    return L

def Display_Decision(x, y, m, decision, setosa, virginica):
    #separating setosa and virginica samples into different arrays 
    for i in range(m):
        if y[i,0] == 1:
            virginica.append(x[i, (1,2)])
        else:
            setosa.append(x[i, (1,2)])
    setosa = np.array(setosa)
    virginica = np.array(virginica)
    plt.figure(1)
    #plotting the samples
    plt.scatter(setosa[:, 0], setosa[:, 1],c = "r")
    plt.scatter(virginica[:, 0], virginica[:, 1], c = "purple")
    #equation of decision boundary
    x1 = [x[:,1].min(), x[:,1].max()]
    x1 = np.array(x1)
    m = -theta[1,0]/theta[2,0]
    c = -theta[0,0]/theta[2,0]
    theta
    x2 = m*x1 + c
    plt.plot(x1, x2)
    plt.figure(2)
    plt.plot(Loss_Error)

#Gradient Descent
def GDescent(x, y , yhat, theta, iterations, learning_rate):
    for i in range(iterations):
        theta_gradient = np.dot(x.T, (yhat - y))
        theta = theta - np.dot(theta_gradient, learning_rate)
        yhat = Sigmoid(np.dot(x, theta))
        Loss_Error.append(LogCost(yhat, y))
    return theta, yhat, Loss_Error

#Decision function 
def Decision(yhat, m):
    decision = []
    for i in range(m):
        if yhat[i] > 0.5:
            decision.append(1)
        else:
            decision.append(0)
    decision = np.array(decision)
    return decision

#Accuarcy function
def Accuracy(y, decision, m):
    correct = 0
    for i in range(m):
        if decision[i] == y[i,0]:
            correct +=1
    return (correct/m)*100

#implementing the model
theta, yhat, Loss_Error = GDescent(x, y, yhat, theta, 3000, 0.01)
decision = Decision(yhat, m)
Display_Decision(x, y, m, decision, setosa, virginica)
acc = Accuracy(y, decision, m)
print(f"Accuracy of the model:{acc}")
loss = LogCost(yhat, y)
print(f"Final cost: {loss}")

#evaluating the model for test data
test_x = np.array(test_features)
test_y = np.array(test_labels)
test_x = (test_x - test_x.min()) / (test_x.max() - test_x.min())
test_m = len(test_x)
test_y = np.reshape(test_y, (test_m, 1))
test_x = np.append(np.ones((test_m,1)), test_x, axis= 1)


test_yhat = Sigmoid(np.dot(test_x, theta))

test_decision = Decision(test_yhat, test_m)
test_accuracy = Accuracy(test_y, test_decision, test_m)
print(f"accuracy for test data: {test_accuracy}")