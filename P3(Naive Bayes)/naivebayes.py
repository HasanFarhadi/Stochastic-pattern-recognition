# -*- coding: utf-8 -*-
"""NaiveBayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Rku3Le8IqHiEP7gb8sG1AV3SF3SaYv1
"""

import numpy as np
import pandas as pd
import string as str
import copy
from sklearn.model_selection import train_test_split
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords');

#Loading Data into train and test dataframes
def Load_Train_test(input_path):
    data = pd.read_csv(input_path, sep='\t', lineterminator='\n', header= None, names = ['Sentiment', 'Label'])

    x, x_test, y, y_test = train_test_split(data.iloc[: , 0], data.iloc[: , 1], test_size=0.2, random_state=42, stratify=data.iloc[: , 1])
    train = pd.concat([y,x], axis = 1)
    test = pd.concat([y_test, x_test], axis = 1)
    return train, test

def NaiveBayes_Model(train):
    #Splitting full Sentiment into lowercase words without punctutaion marks

    train['Sentiment'] = train['Sentiment'].str.replace('\W', ' ')
    train['Sentiment'] = train['Sentiment'].str.lower()
    train['Sentiment'] = train['Sentiment'].str.split()

    
    #making a list of all possible unique words
    features = []
    for Sentiment in train['Sentiment']:
        for word in Sentiment:
            features.append(word)
    features = list(set(features))

    
    #making another list consisting features which are uncommon words 
    non_stopword_features = features.copy()

    for stopword in stopwords.words('english'):
        if stopword in non_stopword_features:
            non_stopword_features.remove(stopword)


    #making a ledger of number of occurances of each feature in every Sentiment
    ledger = {word: [0]*len(train['Sentiment']) for word in features}

    for index, Sentiment in enumerate(train['Sentiment']):
        for word in Sentiment:
            ledger[word][index] += 1


    ledger = pd.DataFrame(ledger)


    ledger = pd.DataFrame.reset_index(ledger, drop = True)
    train = pd.DataFrame.reset_index(train, drop = True)

    train = pd.concat([train, ledger], axis = 1)
    
    # A  prior prob

    positive = train[train['Label'] == 1]
    negative = train[train['Label'] == 0]

    Prob_positive = len(positive) / len(train)
    Prob_negative = len(negative) / len(train)

    positive_sentiment_length = positive['Sentiment'].apply(len)
    N_positive = positive_sentiment_length.sum()

    negative_sentiment_length = negative['Sentiment'].apply(len)
    N_negative = negative_sentiment_length.sum()

    N_features = len(features)


# P(B|A) prior prob
    Prior_Prob_positive = {word:0 for word in features}
    Prior_Prob_negative = {word:0 for word in features}
    negative_instances = {word:0 for word in features}
    positive_instances = {word:0 for word in features}

    for word in features:
        negative_instances[word] = negative[word].sum()
        Prior_Prob_negative[word] = (negative_instances[word] + 1) / (N_negative + N_features)

        positive_instances[word] = positive[word].sum()
        Prior_Prob_positive[word] = (positive_instances[word] + 1) / (N_positive + N_features)

    return Prior_Prob_positive, Prior_Prob_negative, non_stopword_features, Prob_positive, Prob_negative

def Predict(message, prior_probability_positive, prior_probability_negative, features, prob_positive, prob_negative):
    Posterior_Prob_positive = 1
    Posterior_Prob_negative = 1

    message = message.replace('\W', ' ')
    message = message.lower().split()
#Posterior Prob
    for word in message:
        if word in non_stopword_features:
            Posterior_Prob_positive = Posterior_Prob_positive + -np.log(Prior_Prob_positive[word])
            Posterior_Prob_negative = Posterior_Prob_negative + -np.log(Prior_Prob_negative[word])

    Posterior_Prob_positive = Posterior_Prob_positive * Prob_positive
    Posterior_Prob_negative = Posterior_Prob_negative * Prob_negative


    if Posterior_Prob_positive < Posterior_Prob_negative:
        result = 1
    else:
        result = 0
    
    return result

#Test accuracy for test data
def Test_Accuracy(Test, prior_probability_positive, prior_probability_negative, features, prob_positive, prob_negative):

    count = 0

    Test = np.array(Test)

    for i in range(len(Test)):
        if Test[i, 0] == Predict(Test[i, 1], Prior_Prob_positive, Prior_Prob_negative, non_stopword_features, Prob_positive, Prob_negative):
            count += 1
    accuracy = (count / len(Test)) * 100
    return accuracy

train, test = Load_Train_test("/content/drive/MyDrive/Depo/PatternHW/HW3/yelp_labelled.txt")
train
Prior_Prob_positive, Prior_Prob_negative, non_stopword_features, Prob_positive, Prob_negative = NaiveBayes_Model(train)
yelp_acc = Test_Accuracy(test, Prior_Prob_positive, Prior_Prob_negative, non_stopword_features, Prob_positive, Prob_negative)
print(f"Accuracy over yelp reviews: {yelp_acc}")

train, test = Load_Train_test("/content/drive/MyDrive/Depo/PatternHW/HW3/amazon_cells_labelled.txt")
train
Prior_Prob_positive, Prior_Prob_negative, non_stopword_features, Prob_positive, Prob_negative = NaiveBayes_Model(train)
amazon_acc = Test_Accuracy(test, Prior_Prob_positive, Prior_Prob_negative, non_stopword_features, Prob_positive, Prob_negative)
print(f"Accuracy over amazon reviews: {amazon_acc}")

train, test = Load_Train_test("/content/drive/MyDrive/Depo/PatternHW/HW3/imdb_labelled.txt")
train
Prior_Prob_positive, Prior_Prob_negative, non_stopword_features, Prob_positive, Prob_negative = NaiveBayes_Model(train)
imdb_acc = Test_Accuracy(test, Prior_Prob_positive, Prior_Prob_negative, non_stopword_features, Prob_positive, Prob_negative)
print(f"Accuracy over imdb reviews: {imdb_acc}")

overall_acc = (imdb_acc + amazon_acc + yelp_acc) / 3
print(f"Overall Accuracy: {overall_acc}")